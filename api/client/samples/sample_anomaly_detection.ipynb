{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Tool\n",
    "In this notebook, we outline a process for running a simple statistical analysis for anomalies on a set of geospatial data. Below, you can find some helpful frameworks for pulling a batch of data for multiple regions, transforming that data into a more statistically significant format, identifying key properties of that data, and then identifying if recent values are anomalous according to two sigma significance. This code is extendable to include different tests for significance, different data sets and different use cases.\n",
    "\n",
    "## Set up the coding environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from api.client.gro_client import GroClient\n",
    "from datetime import datetime, date, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GROAPI_TOKEN = os.environ['GROAPI_TOKEN']\n",
    "API_HOST = 'api.gro-intelligence.com'\n",
    "gclient = GroClient(API_HOST, GROAPI_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 district ids:\n",
      "[102446, 102454, 102459, 102463, 102453, 102449, 102458, 102451, 102448, 102457, 102447, 102465, 102467, 102462, 102464, 102455, 102468, 102461, 102450, 102452, 102460, 102456, 102466, 100022792]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#data_name = \"Salta Province in Brazil Rainfall Anomalies\"\n",
    "data_name = \"Salta Province in Argentina Temperature Anomalies\"\n",
    "salta_region_id = 10152\n",
    "districts_list = gclient.lookup('regions', salta_region_id)['contains']\n",
    "print(len(districts_list), \"district ids:\" )\n",
    "print(districts_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need a function to help us the best data series for multiple locations\n",
    "\n",
    "We want to choose the sereis with the best rank so we use this handy method to retrieve the best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def best_series_picker(selection):\n",
    "    '''\n",
    "    chooses the series with the greatest density and history of datapoints\n",
    "    '''\n",
    "    temp_series = (next(gclient.rank_series_by_source(selection)))\n",
    "    temp_series = gclient.get_data_series(**temp_series)\n",
    "    temp_series = (temp_series[0])\n",
    "    return temp_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the ids necessary to pull a data series at gro-intelligence.com by searching for the terms you need. in this case we are going to look at Land temperature (daytime, modeled) from the MOD11 Satellite housed on gro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_land_temp(regions_list):\n",
    "    '''\n",
    "    # Land temperature (daytime, modeled) - Temperature - (MOD11 LST)\n",
    "    '''\n",
    "    selections = []\n",
    "    for region in regions_list:\n",
    "        '''\n",
    "        ids from gro-intelligence.com\n",
    "        '''\n",
    "        selections.append(best_series_picker([{'metric_id': 2540047, 'item_id': 3457, 'region_id': region}])) #you can get these id's from our web app at gro-intelligence.com\n",
    "    \n",
    "    output = []\n",
    "    for set_of_ids in selections:\n",
    "        output.append(gclient.get_data_points(**set_of_ids))\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to pull data from the Gro API and examine what we have. (This may take a few seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_data_array = get_land_temp(districts_list)\n",
    "# We will use all_data_array later!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on that data by printing one data point! (if you try to print the whole thing the notebook will crash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start_date': '2019-12-10T00:00:00.000Z', 'end_date': '2019-12-10T00:00:00.000Z', 'value': 37.0214822237871, 'input_unit_id': 36, 'input_unit_scale': 1, 'unit_id': 36, 'metric_id': 2540047, 'item_id': 3457, 'region_id': 102446, 'frequency_id': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "first_location_data = all_data_array[0]\n",
    "first_data_point = first_location_data[-1]\n",
    "print(first_data_point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at what we are working with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most recently reported value 37.0214822237871\n",
      "item name: Land temperature (daytime, modeled)\n",
      "unit name: Celsius\n",
      "name of region: Anta\n",
      "name of metric: Temperature\n",
      "last reported data point:  on 2019-12-10 \n",
      "frequency reported: daily\n",
      "time period (years): 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "most_recent_value = (first_location_data[-1]['value'])\n",
    "item = gclient.lookup('items',first_location_data[0]['item_id'])['name']\n",
    "unit = gclient.lookup('units',first_location_data[0]['unit_id'])['name']\n",
    "region = gclient.lookup('regions',first_location_data[0]['region_id'])['name']\n",
    "metric_id = first_location_data[0]['metric_id']\n",
    "metric_name = gclient.lookup('metrics',metric_id)['name']\n",
    "time_stamp = \" on \" + first_location_data[-1]['end_date'][:10] + \" \"\n",
    "frequency = gclient.lookup('frequencies',first_location_data[0]['frequency_id'])['name']\n",
    "years = datetime.strptime(first_location_data[-1]['end_date'][:10], '%Y-%m-%d').year - datetime.strptime(first_location_data[0]['end_date'][:10], '%Y-%m-%d').year\n",
    "\n",
    "print(\"most recently reported value\", most_recent_value)\n",
    "print(\"item name:\", item)\n",
    "print(\"unit name:\", unit)\n",
    "print(\"name of region:\", region)\n",
    "print(\"name of metric:\", metric_name)\n",
    "print(\"last reported data point:\", time_stamp)\n",
    "print(\"frequency reported:\", frequency)\n",
    "print(\"time period (years):\", years)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will want to give our users friendly wording when they read their anomaly alerts\n",
    "this is optional but for many readers \"Water Loss\" makes more sense than \"Evapotranspiration\"\n",
    "the variable data is defined as one series from the all_data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_if_geospatial(data):\n",
    "    metric_id = data[0]['metric_id']\n",
    "    if(metric_id==2100031 or metric_id ==15531082 or metric_id == 430029 or metric_id ==2010042 or metric_id == 2540047):\n",
    "        is_geospatial = True\n",
    "    else:\n",
    "        is_geospatial =  False\n",
    "    if(metric_id==2100031):# Trmm\n",
    "        metric_name = 'Rainfall'\n",
    "    elif(metric_id==15531082):# Soil Moisture\n",
    "        metric_name = 'Soil Moisture'\n",
    "    elif(metric_id==430029):# NDVI\n",
    "        metric_name = 'Vegetation Health'\n",
    "    elif(metric_id==2010042):# ETA\n",
    "        metric_name = 'Water Loss'\n",
    "    elif(metric_id==2540047):# Temp\n",
    "        metric_name = 'Temperature'\n",
    "    else:\n",
    "        metric_name = ''\n",
    "\n",
    "    return is_geospatial, metric_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geospatial data: True\n",
      "type: Temperature\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Lets try it out on our first data location\n",
    "is_geospatial, new_metric_name = check_if_geospatial(first_location_data)\n",
    "print(\"geospatial data:\", is_geospatial)\n",
    "print(\"type:\", new_metric_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data for more significant signals\n",
    "\n",
    "For some data series we dont want to look at values as frequently as they are reported, instead we want to look at 7-day cumulative or 7-day average values. Here we will modify those specified series and Temperature is one of them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated frequency: 7-day average\n",
      "most recent 7-day average value: 31.085297007615953\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def transform_by_data_type(df, current_value, frequency):\n",
    "    '''\n",
    "    change frequency for certain weather items\n",
    "    '''\n",
    "    metric_id = df.loc[0,'metric_id']\n",
    "    if(metric_id == 2540047 or metric_id == 15531082): # 2540047 is Land temp, 15531082 is soil moisture (MEAN)\n",
    "        df = df.fillna(df.mean())\n",
    "        df['value'] = df['value'].rolling(7).mean()\n",
    "        frequency = \"7-day average\"\n",
    "    elif( metric_id == 2100031):\n",
    "        df = df.fillna(0)\n",
    "        df['value'] = df['value'].rolling(7).sum() # 2100031 is rainfall (SUM)\n",
    "        frequency = \"7-day cumulative\"\n",
    "    else:\n",
    "        return df, current_value, frequency\n",
    "    df = df.iloc[::7, :]\n",
    "    df.index = pd.RangeIndex(len(df.index))\n",
    "    current_value = df.iloc[-1]['value']\n",
    "    return df, current_value, frequency\n",
    "\n",
    "\n",
    "# Get percent change from average\n",
    "df = pd.DataFrame(first_location_data)\n",
    "\n",
    "df, current_value, frequency = transform_by_data_type(df, most_recent_value, frequency)\n",
    "\n",
    "print(\"updated frequency:\", frequency)\n",
    "print(\"most recent 7-day average value:\", current_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a significance / anomaly test\n",
    "\n",
    "Now we need to find the lower and upper bounds for our testing for significance, we have chosen to consider two standard deviations from the mean. After that we will use the bounds to test for significant deviations from the mean and record highs and lows. This test could be replaced with any other test for significance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower bound: 16.156849838891347\n",
      "upper bound: 35.78526612313051\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_lower_and_upper(df,standard_devs):\n",
    "    '''\n",
    "    Standard Deviation Calculations for Alerts:\n",
    "    Using standard deviations we cand find out if recent numbers\n",
    "    reported to the user are abnormal or to be expected\n",
    "    Returns upper and lower bounds for a series and a given number of standard deviations\n",
    "    '''\n",
    "    mean = df.mean()\n",
    "    std = df.std()\n",
    "    lower_bound = (mean - (standard_devs * std))\n",
    "    upper_bound = (mean + (standard_devs * std))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "\n",
    "df_lower_upper = df['value']\n",
    "standard_deviations = 2\n",
    "lower_bound, upper_bound = find_lower_and_upper(df_lower_upper,standard_deviations)\n",
    "\n",
    "print(\"lower bound:\", lower_bound)\n",
    "print(\"upper bound:\", upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test result: Not significant for period: 19 years \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def selected_tests(df,lower_bound,upper_bound,current_value):\n",
    "    is_significant = False\n",
    "    statement = \"\"\n",
    "    if(current_value == df[\"value\"].max()):\n",
    "        is_max = True\n",
    "        statement = \"Record High in \"\n",
    "    elif(current_value == df[\"value\"].min()):\n",
    "        is_max = True\n",
    "        statement = \"Record Low in \"\n",
    "    elif(current_value<=lower_bound):\n",
    "        is_significant = True\n",
    "        statement = \"Significant low in \"\n",
    "    elif(current_value>=upper_bound):\n",
    "        is_significant = True\n",
    "        statement = \"Significant high in \"\n",
    "    else:\n",
    "        statement = \"Not significant for period: \"\n",
    "    return is_significant, statement\n",
    "\n",
    "\n",
    "is_significant, statement = selected_tests(df,lower_bound,upper_bound,current_value)\n",
    "\n",
    "statement += str(years) + \" years \"\n",
    "current_value = ((round(current_value,2)))\n",
    "\n",
    "print(\"test result:\", statement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to contextualize this number by saying exactly what the deviation from the mean looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent deviation: 20% above average\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_percent_change_statement(number, avg, metric_id):\n",
    "    if(metric_id == 2010042): #ETA\n",
    "        percent_val = number/100\n",
    "        deviation = number - 100\n",
    "    elif(metric_id == 430029): #NDVI\n",
    "        percent_val = abs(number)\n",
    "        deviation = number\n",
    "    else:\n",
    "        deviation = (number) - (avg)\n",
    "        if(avg!=0):\n",
    "            percent_val = (abs(deviation) / abs(avg))\n",
    "        else:\n",
    "            return ''\n",
    "    # Format string response\n",
    "    if(deviation>=0):\n",
    "        statement = str(\"{0:.0%}\".format(percent_val)) + \" above average\"\n",
    "    else:\n",
    "        statement = str(\"{0:.0%}\".format(percent_val)) + \" below average\"\n",
    "\n",
    "    return statement\n",
    "\n",
    "mean = df[\"value\"].mean()\n",
    "percent_change = get_percent_change_statement(current_value, mean, metric_id)\n",
    "\n",
    "print(\"percent deviation:\", percent_change)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "After walking through the process we want to add some methods so we can integrate this into our workflow and use it for many data series at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def current_is_significant(data):\n",
    "    '''\n",
    "    Returns bool True if the latest value is the global max\n",
    "    '''\n",
    "    current_value = (data[-1]['value'])\n",
    "    frequency = gclient.lookup('frequencies',data[0]['frequency_id'])['name']\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df, current_value, frequency = transform_by_data_type(df, current_value, frequency)\n",
    "    current_value = df.iloc[-1]['value']\n",
    "\n",
    "    df_lower_upper = df['value']\n",
    "    standard_deviations = 2\n",
    "    lower_bound, upper_bound = find_lower_and_upper(df_lower_upper,standard_deviations)\n",
    "    \n",
    "    is_significant, statement = selected_tests(df,lower_bound,upper_bound,current_value)\n",
    "    \n",
    "    # Get frequency\n",
    "    years = datetime.strptime(data[-1]['end_date'][:10], '%Y-%m-%d').year - datetime.strptime(data[0]['end_date'][:10], '%Y-%m-%d').year\n",
    "    statement += str(years) + \" years \"\n",
    "    return is_significant, statement, ((round(current_value,2)))\n",
    "\n",
    "\n",
    "def return_alerts(data):\n",
    "    alert = \"\"\n",
    "    '''\n",
    "    alert if current data is significant in gro series\n",
    "    make sure there was data in the original request before running the rest\n",
    "    '''\n",
    "    #try:\n",
    "    # Make sure there are more than 3 data points\n",
    "    data[3]\n",
    "    if(data[-1][\"value\"]==data[-6][\"value\"]):\n",
    "        return \"\"\n",
    "    # Make sure the most recent reporting date is within the last 30 days\n",
    "    current_date = datetime.now()\n",
    "    try:\n",
    "        most_recent = data[-1]['reporting_date']\n",
    "    except:\n",
    "        most_recent = data[-1]['end_date']\n",
    "    data_date = datetime.strptime(most_recent[:10], '%Y-%m-%d')\n",
    "    delta = current_date-data_date\n",
    "    days = int(str(delta).split()[0])\n",
    "    if(days>=30):\n",
    "        return \"\"\n",
    "    \n",
    "    # alert if current is max\n",
    "    bin_val1,statement1,val1 = current_is_significant(data)\n",
    "    if(bin_val1):\n",
    "        alert += \" - \" + statement1\n",
    "    \n",
    "    ''' You can add tests for seasonal significance, tests for alternate distributions etc.\n",
    "    bin_val2,statement2,val2 = YOUR_TEST_HERE(data)  \n",
    "    if(bin_val2):\n",
    "        alert += \" - \" + statement2\n",
    "    '''\n",
    "    return alert\n",
    "\n",
    "\n",
    "def string_formatting(data, number):\n",
    "    try:\n",
    "        # Make sure there was data returned \n",
    "        most_recent_value = (data[number]['value'])\n",
    "        \n",
    "        # Provide metadata for the reader from gro api client and format into human readable string\n",
    "        item = gclient.lookup('items',data[0]['item_id'])['name']\n",
    "        unit = gclient.lookup('units',data[0]['unit_id'])['name']\n",
    "        region = gclient.lookup('regions',data[0]['region_id'])['name']\n",
    "        metric_id = data[0]['metric_id']\n",
    "        metric_name = gclient.lookup('metrics',metric_id)['name']\n",
    "        time_stamp = \" on \" + data[number]['end_date'][:10] + \" \"\n",
    "        frequency = gclient.lookup('frequencies',data[0]['frequency_id'])['name']\n",
    "        \n",
    "    except (TypeError, KeyError):\n",
    "        print(\"type error or key error in string_formatting\")\n",
    "        return \"\", ''\n",
    "\n",
    "    # Get percent change from average\n",
    "    df = pd.DataFrame(data)\n",
    "    df, current_value, frequency = transform_by_data_type(df, most_recent_value, frequency)\n",
    "    mean = df[\"value\"].mean()\n",
    "    percent_change = get_percent_change_statement(most_recent_value, mean, metric_id)\n",
    "\n",
    "    # Check if this is a geospatial data series\n",
    "    is_geospatial, new_metric_name = check_if_geospatial(data)\n",
    "\n",
    "    # If this IS a geospatial metric\n",
    "    if(is_geospatial):\n",
    "        #create string\n",
    "        significance_headline = region+ \" - \" + new_metric_name + \": \" + percent_change\n",
    "\n",
    "    # If this is NOT a geospatial metric\n",
    "    else:\n",
    "        # Make sure it exists\n",
    "        if(most_recent_value):x\n",
    "            most_recent_value = round(most_recent_value,2)\n",
    "        else:\n",
    "            most_recent_value = 0\n",
    "\n",
    "        #Adds commas and gets rid of decimals\n",
    "        if(most_recent_value>100):\n",
    "            most_recent_value = (format(int(most_recent_value), ',d'))\n",
    "        else:\n",
    "            most_recent_value = str(most_recent_value)\n",
    "        # create string\n",
    "        item_metric = item + \" - \" + metric_name\n",
    "        significance_headline = region+ \" - \" + item_metric + \": \" + most_recent_value + \" \" + unit + \", \" + percent_change\n",
    "    # removes words within parentheses for easier reading\n",
    "    significance_headline = (re.sub(r\" ?\\([^)]+\\)\", \"\", significance_headline))\n",
    "    return significance_headline\n",
    "\n",
    "\n",
    "def work_to_distribute(data):\n",
    "    '''\n",
    "    Method run by workers, this will run each of the tests in return_alerts\n",
    "    '''\n",
    "    string_add = \"\"\n",
    "    data_line_2 = return_alerts(data)\n",
    "    if(data_line_2!=\"\"):\n",
    "        data_line_1 = string_formatting(data,-1)\n",
    "        string_add += (data_line_1 + data_line_2 + \"\\n\")\n",
    "    return string_add\n",
    "\n",
    "\n",
    "def recent_data_to_string(array, data_name):\n",
    "    '''\n",
    "    Takes array of data series and a category name and returns any alerts string formatted under a header\n",
    "    '''\n",
    "    string = \"\"\n",
    "    string_add = \"\"\n",
    "    string_list = []\n",
    "    if(array != None):\n",
    "        try:\n",
    "            for ele in array:\n",
    "                string_list.append(work_to_distribute(ele))\n",
    "        except IndexError:\n",
    "            print(\"IndexError in recent_data_to_string for \" + data_name)\n",
    "            return string_add\n",
    "            \n",
    "        for x in string_list:\n",
    "            string_add += x\n",
    "        if(string_add!=\"\"):\n",
    "            string = data_name\n",
    "        else:\n",
    "            string = \"\"\n",
    "\n",
    "    data_statement = string + \" \\n\" + string_add\n",
    "\n",
    "    return data_statement\n",
    "\n",
    "\n",
    "argentina_string = recent_data_to_string(all_data_array, data_name)\n",
    "print(argentina_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing new data\n",
    "\n",
    "Now that we have our alerts from Argentina about Land Temperature (If the cell above prints nothing, that means there were no alerts for significant temperature in Argentina), we will try brazil NDVI (Normalized Difference Vegetation Index) anomalies as well. Our data source for NDVI allows us to estimate the amound of chlorophyll in plants in a given region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110117, 110031, 110130, 110097, 110120, 110059, 110124, 110047, 110041, 110118, 110055, 110061, 110084, 110082, 110096, 110077, 110109, 110100, 109997, 110045, 110021, 110074, 109998, 110022, 110024, 110086, 110001, 109995, 110048, 110064, 110004, 110098, 110016, 110037, 110010, 110078, 110107, 109999, 110051, 110017, 110091, 110116, 110023, 110131, 110089, 110065, 110088, 110028, 110002, 110090, 110115, 110092, 110018, 110104, 110052, 110056, 110011, 110106, 110085, 110083, 110035, 110122, 110087, 110121, 110006, 109996, 110068, 110014, 110034, 110057, 110066, 110102, 110042, 110071, 110039, 110053, 110040, 110007, 110094, 110049, 110030, 110073, 110029, 110129, 110076, 110105, 110075, 110067, 110062, 110113, 110114, 110000, 110020, 110046, 110050, 110063, 110101, 110110, 110128, 110036, 110093, 110069, 110012, 110003, 110103, 110080, 110127, 110099, 110060, 110015, 110008, 110033, 110070, 110027, 110095, 110079, 110112, 110019, 110013, 110005, 110125, 110111, 110108, 110072, 110119, 110126, 110032, 110026, 110054, 110123, 110025, 110009, 110044, 110038, 110043, 110081, 110058, 100022399, 100022400, 100022401]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_ndvi_10_year_fixed(regions_list):\n",
    "    '''\n",
    "    # NDVI difference from 10-yr mean (2001-2010) - Vegetation Anomalies\n",
    "    '''\n",
    "    selections = []\n",
    "    for region in regions_list:\n",
    "        '''\n",
    "        ids from gro-intelligence.com\n",
    "        '''\n",
    "        #you can get these id's from our web app at gro-intelligence.com\n",
    "        selections.append(best_series_picker([{'metric_id': 430029, 'item_id': 409, 'region_id': region}]))    \n",
    "    \n",
    "    output = []\n",
    "    for set_of_ids in selections:\n",
    "        output.append(gclient.get_data_points(**set_of_ids))\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Tocantins Province in Brazil\n",
    "tocantins_province_id = 10428\n",
    "districts_list_brazil = gclient.lookup('regions', tocantins_province_id)['contains']\n",
    "print(districts_list_brazil)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Tocantins province NDVI and analyze for significance (this may take a minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tocantins Province in Brazil NDVI Anomalies \n",
      "Wanderlândia - Vegetation Health: 10% above average - Significant high in 19 years \n",
      "Palmeirópolis - Vegetation Health: 8% above average - Significant high in 19 years \n",
      "Angico - Vegetation Health: 8% above average - Significant high in 19 years \n",
      "Luzinópolis - Vegetation Health: 8% above average - Significant high in 19 years \n",
      "Cachoeirinha - Vegetation Health: 9% above average - Significant high in 19 years \n",
      "Riachinho - Vegetation Health: 9% above average - Significant high in 19 years \n",
      "Darcinópolis - Vegetation Health: 10% above average - Significant high in 19 years \n",
      "Maurilândia do Tocantins - Vegetation Health: 10% above average - Significant high in 19 years \n",
      "Santa Terezinha do Tocantins - Vegetation Health: 13% above average - Significant high in 19 years \n",
      "São Bento do Tocantins - Vegetation Health: 9% above average - Significant high in 19 years \n",
      "Ananás - Vegetation Health: 11% above average - Significant high in 19 years \n",
      "Ponte Alta do Norte - Vegetation Health: 8% above average - Significant high in 19 years \n",
      "Mosquito - Vegetation Health: 11% above average - Significant high in 19 years \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#take out the ids for non districts:\n",
    "districts_list_brazil = districts_list_brazil[:-3]\n",
    "\n",
    "ndvi_array = get_ndvi_10_year_fixed(districts_list_brazil)\n",
    "brazil_string = recent_data_to_string(ndvi_array, \"Tocantins Province in Brazil NDVI Anomalies\")\n",
    "print(brazil_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
